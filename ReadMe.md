# ğŸ§  LLM-Engineering

Welcome to **LLM-Engineering** â€” a personal collection of AI/LLM-powered projects built to explore, experiment, and engineer real-world applications using large language models. This repository serves as a showcase of practical implementations, ranging from web automation to knowledge processing â€” all powered by modern LLMs like LLaMA 3, via local or API-based interfaces.

> âš™ï¸ Built with a strong focus on clean code, practical use cases, and minimal dependencies.

---

## ğŸ“¦ Projects

### 1. ğŸ•¸ï¸ AI Website Summarizer

#### Description

**AI Website Summarizer** is a simple web application that allows users to input any website URL and get a clean, concise summary of the site's content â€” powered by a locally running LLM via Ollama.

> Think of it as ChatGPT, but for any website, and running entirely on your machine.

#### ğŸ”‘ Features

- FastAPI backend + HTML/CSS frontend
- BeautifulSoup used to scrape and clean content
- OpenAI-compatible LLaMA 3 model via [Ollama](https://ollama.com/)
- Markdown + typing animation for responses
- Futuristic UI with reset and re-summarize functionality

#### ğŸš€ Quick Start

```bash
# Clone the repo
git clone https://github.com/your-username/LLM-Engineering.git
cd LLM-Engineering/website-summarizer

# (Optional) Create a virtual environment
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start the Ollama model
ollama run llama3

# Run the backend
uvicorn main:app --reload
```

### ğŸ›ï¸ Project 2: Store Assistant AI Chatbot

#### Description

This project is a simple AI-powered chatbot designed to assist customers in a clothing store. The assistant helps answer customer queries strictly related to the store and gently encourages the purchase of discounted itemsâ€”especially hats, which are 60% off!

The chatbot uses [OpenAI-compatible](https://platform.openai.com/docs/api-reference/chat) models (e.g., `llama3.2`) running locally via [Ollama](https://ollama.com), and is served through an interactive Gradio interface.

---

#### ğŸ’¡ Features

- AI-powered chatbot for clothing store assistance.
- Highlights promotional items: 
  - ğŸ§¢ Hats â€“ 60% off!
  - ğŸ‘• Most other items â€“ 50% off.
- Responds only to store-related questions.
- Politely deflects unsupported queries (e.g., "Do you sell belts?").
- Encourages upselling where appropriate.

---

#### ğŸ”§ Requirements

- Python 3.8+
- [Ollama](https://ollama.com) running locally with a model like `llama3.2`
- Required Python packages:
  - `openai`
  - `gradio`

### ğŸ“¦ Installation

```bash
pip install -r requirements.txt
```

### Project #3 â€“ DocDetect & Extract

**DocDetect & Extract** is a local AI-powered assistant built with **Gradio**, **Ollama**, and **LLaMA 3.2** that can:

- Preview uploaded PDF documents  
- Classify whether the document is an **Invoice** or a **Statement**  
- Extract important fields into a **predefined structured JSON format**  
- Validate the extracted data against a JSON Schema

---

#### Features
- ğŸ“„ **PDF Preview** â€“ See the uploaded document before processing.
- ğŸ§  **Local AI Processing** â€“ Uses **Ollama** and **LLaMA 3.2** running locally at `http://localhost:11434/v1`.
- ğŸ” **Document Classification** â€“ Detects if the document is an invoice or statement.
- ğŸ“Š **Structured Data Extraction** â€“ Outputs key fields (e.g., vendor, invoice number, dates, line items).
- âœ… **Schema Validation** â€“ Ensures the AI output matches the expected JSON structure.

---

#### Tech Stack
- **Python 3.10+**
- [Gradio](https://gradio.app/) â€“ Web UI
- [PyMuPDF](https://pymupdf.readthedocs.io/) â€“ PDF preview & text extraction
- [Ollama](https://ollama.com/) â€“ Local LLaMA model hosting
- [JSON Schema](https://json-schema.org/) â€“ Validation

---

#### Installation

#### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/yourusername/docdetect-extract.git
cd docdetect-extract
```



### Project 3 â€“ AI Farming Advisor (LLM + Quantization + Edge Deployment)

#### ğŸ“Œ Overview
This project demonstrates how to **run a quantized Large Language Model (LLM) on resource-constrained devices** such as laptops, smartphones, or small edge devices.  
It uses the **Microsoft Phi-3 Mini Instruct** model with **8-bit quantization** (via `bitsandbytes`) to make inference lighter and faster, while maintaining useful performance.  

The chatbot is designed as an **AI Farming Advisor** that can give contextual answers, enhanced by **real-time location and weather data**.  

---

#### ğŸ¯ Key Features
- âœ… **Quantization (8-bit)** for optimized model loading on GPUs/CPUs with graceful fallbacks  
- âœ… **Custom System Prompt** â†’ â€œAI Farming Advisorâ€  
- âœ… **Context Integration** â†’ Uses IP-based location + OpenWeatherMap API for real-time weather info  
- âœ… **Streaming Chat UI** â†’ Built with **Gradio** for a responsive chatbot experience  
- âœ… **Lightweight Deployment** â†’ Potential to run on smaller devices (edge AI vision)  

---

#### âš™ï¸ Tech Stack
- [Transformers](https://huggingface.co/docs/transformers) â€“ Model loading & inference  
- [Bitsandbytes](https://github.com/TimDettmers/bitsandbytes) â€“ 8-bit quantization  
- [Torch](https://pytorch.org/) â€“ Model execution (GPU/CPU)  
- [Gradio](https://gradio.app/) â€“ Interactive chatbot UI  
- [Geocoder](https://geocoder.readthedocs.io/) â€“ Location detection  
- [OpenWeatherMap API](https://openweathermap.org/api) â€“ Weather integration  

---

#### ğŸš€ Pipeline
1. **Install Dependencies**  
   ```bash
   pip install torch transformers accelerate bitsandbytes gradio sentencepiece requests geocoder
```