{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 126399,
     "status": "ok",
     "timestamp": 1755321144729,
     "user": {
      "displayName": "Mohammed Mashaq",
      "userId": "07101950052469826127"
     },
     "user_tz": -330
    },
    "id": "fQ8JFtN2o0bY",
    "outputId": "eee660bf-5a10-44ab-c4c6-4f5d3f94b8ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes sentencepiece torch requests geocoder\n",
    "!pip install -q --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxeIS88FhE2Q"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Tuple, Optional\n",
    "from transformers import TextIteratorStreamer\n",
    "import threading\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gradio as gr\n",
    "import requests\n",
    "import geocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VQQgUMYhIBp"
   },
   "source": [
    "Complete Gradio chat app for Microsoft Phi-3 Mini (Instruct) with 8-bit quantization.\n",
    "- Uses bitsandbytes 8-bit loading when available.\n",
    "- Falls back to FP16 on CUDA or FP32 on CPU with a clear warning.\n",
    "- Applies the chat template with a system prompt: \"You are an AI Farming Advisor.\"\n",
    "\n",
    "Install (pick the cuda variant that matches your system):\n",
    "    pip install torch --index-url https://download.pytorch.org/whl/cu121  # or cpu\n",
    "    pip install transformers accelerate bitsandbytes gradio sentencepiece\n",
    "    # On Windows, prefer a CUDA-specific bitsandbytes build if needed, e.g.:\n",
    "    # pip install bitsandbytes-cuda121  # or cuda118/cuda117 depending on your drivers\n",
    "\n",
    "Run:\n",
    "    python phi3_mini_gradio_8bit.py\n",
    "Then open the printed local URL in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCy7peIjhPUC"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = os.environ.get(\"PHI3_MODEL_ID\", \"microsoft/Phi-3-mini-4k-instruct\")\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "WEATHER_API_KEY = userdata.get('WEATHER_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e1JV2JyhXQR"
   },
   "source": [
    "### Model / Tokenizer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "972a0b073e674b2187439b8fdfeefa26",
      "962d2fc272fc47e1b1f5c2248b1ce466",
      "924e95bb398e4716864cfe32e5dfc313",
      "0f57edd03b564666bbd81c17a5395243",
      "bd6fb6f6644f4f62ae6a9ea9cbd73828",
      "85f168552bd4423e8ad6ca00e7e3673d",
      "7970085acc5a489380e030f8fa619d2f",
      "ef8f4d7147244df18d0a8c0613df21dc",
      "29b4c3ca722343c1bd6fd4f09291f4e7",
      "1cd9f62721a345c1818a6432d302e181",
      "4ba92c68c0be415c888ae712a554e131"
     ]
    },
    "executionInfo": {
     "elapsed": 40251,
     "status": "ok",
     "timestamp": 1755323357729,
     "user": {
      "displayName": "Mohammed Mashaq",
      "userId": "07101950052469826127"
     },
     "user_tz": -330
    },
    "id": "VMoO0PjXhUHA",
    "outputId": "1d33c086-ce0b-4d3f-a8bc-9bcad1bef7c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972a0b073e674b2187439b8fdfeefa26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded microsoft/Phi-3-mini-4k-instruct with dtype: int8 (bitsandbytes) | 8-bit: True\n"
     ]
    }
   ],
   "source": [
    "def load_model_and_tokenizer():\n",
    "    \"\"\"Load Phi-3 Mini in 8-bit if possible, with graceful fallbacks.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "    load_kwargs = {\n",
    "        \"device_map\": \"auto\",\n",
    "    }\n",
    "\n",
    "    # Try 8-bit first\n",
    "    use_8bit = True\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            load_in_8bit=True,  # requires bitsandbytes with GPU support\n",
    "            **load_kwargs,\n",
    "        )\n",
    "        actual_dtype = \"int8 (bitsandbytes)\"\n",
    "    except Exception as e:\n",
    "        warnings.warn(\n",
    "            \"8-bit loading failed (bitsandbytes missing or no GPU support). \"\n",
    "            f\"Falling back. Error: {e}\"\n",
    "        )\n",
    "        use_8bit = False\n",
    "        # Fallback: FP16 if CUDA available, else FP32\n",
    "        if torch.cuda.is_available():\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_ID,\n",
    "                torch_dtype=torch.float16,\n",
    "                **load_kwargs,\n",
    "            )\n",
    "            actual_dtype = \"float16\"\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_ID,\n",
    "                torch_dtype=torch.float32,\n",
    "                device_map={\"\": \"cpu\"},  # ensure CPU if no CUDA\n",
    "            )\n",
    "            actual_dtype = \"float32 (CPU)\"\n",
    "\n",
    "    # Some Phi-3 checkpoints may lack a defined pad token; ensure one exists for batching\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer, actual_dtype, use_8bit\n",
    "\n",
    "\n",
    "model, tokenizer, actual_dtype, using_8bit = load_model_and_tokenizer()\n",
    "print(f\"Loaded {MODEL_ID} with dtype: {actual_dtype} | 8-bit: {using_8bit}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zCSUEZvhp_3"
   },
   "source": [
    "### ---------------------- HELPER FUNCTIONS ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1755321261887,
     "user": {
      "displayName": "Mohammed Mashaq",
      "userId": "07101950052469826127"
     },
     "user_tz": -330
    },
    "id": "uVApPEcThpXG",
    "outputId": "d90ad1c7-b576-44b8-8c56-91151a2aec25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reponse from weather api \n",
      "{'coord': {'lon': -121.1787, 'lat': 45.5946}, 'weather': [{'id': 803, 'main': 'Clouds', 'description': 'broken clouds', 'icon': '04n'}], 'base': 'stations', 'main': {'temp': 20.63, 'feels_like': 20.92, 'temp_min': 19.37, 'temp_max': 21.81, 'pressure': 1012, 'humidity': 83, 'sea_level': 1012, 'grnd_level': 956}, 'visibility': 10000, 'wind': {'speed': 2.68, 'deg': 306, 'gust': 4.47}, 'clouds': {'all': 68}, 'dt': 1755321261, 'sys': {'type': 2, 'id': 2012249, 'country': 'US', 'sunrise': 1755263141, 'sunset': 1755313969}, 'timezone': -25200, 'id': 5756304, 'name': 'The Dalles', 'cod': 200}\n"
     ]
    }
   ],
   "source": [
    "def get_location():\n",
    "    \"\"\"Get approximate user location based on IP address.\"\"\"\n",
    "    g = geocoder.ip('me')\n",
    "    return g.city, g.country, g.lat, g.lng\n",
    "\n",
    "def get_weather(city):\n",
    "    \"\"\"Fetch weather data for the given city using OpenWeatherMap API.\"\"\"\n",
    "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={WEATHER_API_KEY}&units=metric\"\n",
    "    r = requests.get(url)\n",
    "    print(\"reponse from weather api \")\n",
    "    print(r.json())\n",
    "    if r.status_code == 200:\n",
    "        return r.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "city, country, lat, lng = get_location()\n",
    "weather_data = get_weather(city)\n",
    "sys_prompt = f\"You are an AI Farming Advisor. \"\n",
    "if weather_data:\n",
    "    weather_desc = weather_data['weather'][0]['description']\n",
    "    temp = weather_data['main']['temp']\n",
    "    sys_prompt = f\"{sys_prompt} The current location is {city}, {country}, the weather is {weather_desc} and the temperature is {temp}°C. \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOZXpy5Nh0_7"
   },
   "source": [
    "### Chat Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfgB-DoFh6qO"
   },
   "outputs": [],
   "source": [
    "def build_messages(history: List[Tuple[str, str]], user_msg: str):\n",
    "    \"\"\"\n",
    "    Convert Gradio history into a list of chat messages for apply_chat_template.\n",
    "    history: list of (user, assistant) strings. Only non-empty entries are added.\n",
    "    \"\"\"\n",
    "    msgs = [{\"role\": \"system\", \"content\": sys_prompt}]\n",
    "    for user, assistant in history:\n",
    "        if user:\n",
    "            msgs.append({\"role\": \"user\", \"content\": user})\n",
    "        if assistant:\n",
    "            msgs.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "    msgs.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    return msgs\n",
    "\n",
    "\n",
    "def generate_reply(user_msg: str, history: List[Tuple[str, str]]):\n",
    "    messages = build_messages(history, user_msg)\n",
    "\n",
    "    # Build prompt using the chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Create streamer for live output\n",
    "    streamer = TextIteratorStreamer(\n",
    "        tokenizer,\n",
    "        skip_special_tokens=True,\n",
    "        skip_prompt=True\n",
    "    )\n",
    "\n",
    "    # Model.generate runs in a background thread\n",
    "    generation_kwargs = dict(\n",
    "        input_ids=prompt,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        streamer=streamer\n",
    "    )\n",
    "\n",
    "    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # Yield each new token as it's generated\n",
    "    partial_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_text += new_text\n",
    "        yield partial_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiBWuILjiCXa"
   },
   "source": [
    "###Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1755323375127,
     "user": {
      "displayName": "Mohammed Mashaq",
      "userId": "07101950052469826127"
     },
     "user_tz": -330
    },
    "id": "lX9g-ykjiE-q",
    "outputId": "81e2e85a-b506-4bcc-bf6c-d398957e33ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3485452729.py:49: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://e856e2ca6656d48f91.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e856e2ca6656d48f91.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(message: str, history: List[Tuple[str, str]]):\n",
    "    try:\n",
    "        reply = generate_reply(message, history)\n",
    "    except RuntimeError as e:\n",
    "        # Common errors: NaNs from precision mismatch, OOM, etc.\n",
    "        reply = (\n",
    "            \"I hit a runtime error while generating a reply. \"\n",
    "            f\"Details: {e}\\n\\n\"\n",
    "            \"Tips: If you're on CPU, try shorter responses (reduce max_new_tokens). \"\n",
    "            \"If you're on GPU without 8-bit support, install a CUDA-enabled bitsandbytes build.\"\n",
    "        )\n",
    "    return reply\n",
    "\n",
    "\n",
    "def build_ui():\n",
    "    title = f\"Phi-3 Mini Instruct Chat ({actual_dtype})\"\n",
    "    description = (\n",
    "        f\"System prompt: <i>{sys_prompt}</i><br>\"\n",
    "        f\"Model: <code>{MODEL_ID}</code> | Precision: <b>{actual_dtype}</b> | 8-bit: <b>{using_8bit}</b>\"\n",
    "    )\n",
    "\n",
    "    chat = gr.ChatInterface(\n",
    "      fn=predict,\n",
    "      title=title,\n",
    "      description=description,\n",
    "    streaming=True\n",
    "    )\n",
    "\n",
    "    return chat\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    partial_message = \"\"\n",
    "    for chunk in generate_reply(message, chat_history):\n",
    "        partial_message = chunk\n",
    "        yield chat_history + [[message, partial_message]]\n",
    "\n",
    "def build_ui():\n",
    "    title = f\"AI Farming Advisor\"\n",
    "    description = (\n",
    "        f\"Phi-3 Mini Instruct Chat ({actual_dtype})<br>\"\n",
    "        f\"System prompt: <i>{sys_prompt}</i><br>\"\n",
    "        f\"Model: <code>{MODEL_ID}</code> | Precision: <b>{actual_dtype}</b> | 8-bit: <b>{using_8bit}</b>\"\n",
    "    )\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(f\"# {title}\")\n",
    "        gr.Markdown(description)\n",
    "\n",
    "        chatbot = gr.Chatbot()\n",
    "        msg = gr.Textbox(placeholder=\"Type your farming question here...\")\n",
    "\n",
    "        msg.submit(respond, [msg, chatbot], [chatbot])\n",
    "\n",
    "    return demo\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ui = build_ui()\n",
    "    ui.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LAw1JYHZj83Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOWGbgQcPN17lvZy7uapxeS",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f57edd03b564666bbd81c17a5395243": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cd9f62721a345c1818a6432d302e181",
      "placeholder": "​",
      "style": "IPY_MODEL_4ba92c68c0be415c888ae712a554e131",
      "value": " 2/2 [00:39&lt;00:00, 18.54s/it]"
     }
    },
    "1cd9f62721a345c1818a6432d302e181": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29b4c3ca722343c1bd6fd4f09291f4e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ba92c68c0be415c888ae712a554e131": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7970085acc5a489380e030f8fa619d2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85f168552bd4423e8ad6ca00e7e3673d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "924e95bb398e4716864cfe32e5dfc313": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef8f4d7147244df18d0a8c0613df21dc",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_29b4c3ca722343c1bd6fd4f09291f4e7",
      "value": 2
     }
    },
    "962d2fc272fc47e1b1f5c2248b1ce466": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85f168552bd4423e8ad6ca00e7e3673d",
      "placeholder": "​",
      "style": "IPY_MODEL_7970085acc5a489380e030f8fa619d2f",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "972a0b073e674b2187439b8fdfeefa26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_962d2fc272fc47e1b1f5c2248b1ce466",
       "IPY_MODEL_924e95bb398e4716864cfe32e5dfc313",
       "IPY_MODEL_0f57edd03b564666bbd81c17a5395243"
      ],
      "layout": "IPY_MODEL_bd6fb6f6644f4f62ae6a9ea9cbd73828"
     }
    },
    "bd6fb6f6644f4f62ae6a9ea9cbd73828": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef8f4d7147244df18d0a8c0613df21dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
